{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = \"mongo\"\n",
    "DEST = \"landing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectionString='mongodb+srv://CONNECTION_STRING_HERE/\n",
    "database=\"sample_supplies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def child_struct(nested_df):\n",
    "    list_schema = [((), nested_df)]\n",
    "    flat_columns = []\n",
    "\n",
    "    while len(list_schema) > 0:\n",
    "          parents, df = list_schema.pop()\n",
    "          flat_cols = [  col(\".\".join(parents + (c[0],))).alias(\"_\".join(parents + (c[0],))) for c in df.dtypes if c[1][:6] != \"struct\"   ]\n",
    "      \n",
    "          struct_cols = [  c[0]   for c in df.dtypes if c[1][:6] == \"struct\"   ]\n",
    "      \n",
    "          flat_columns.extend(flat_cols)\n",
    "          for i in struct_cols:\n",
    "                projected_df = df.select(i + \".*\")\n",
    "                list_schema.append((parents + (i,), projected_df))\n",
    "    return nested_df.select(flat_columns)\n",
    "\n",
    "def master_array(df):\n",
    "    array_cols = [c[0] for c in df.dtypes if c[1][:5]==\"array\"]\n",
    "    while len(array_cols)>0:\n",
    "        for c in array_cols:\n",
    "            df = df.withColumn(c,explode_outer(c))\n",
    "        df = child_struct(df)\n",
    "        array_cols = [c[0] for c in df.dtypes if c[1][:5]==\"array\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(df, table, pk_col, partition, mode):\n",
    "  mode = mode.lower()\n",
    "  ingestion_time = datetime.now()\n",
    "  dest_path = f\"{SOURCE}/{DEST}/{table}/{ingestion_time.strftime('%Y%m%d')}\"\n",
    "\n",
    "  df = df.withColumn('INGESTION_TIME', F.lit(ingestion_time))\n",
    "\n",
    "  if mode == \"append\":\n",
    "    print(f\"Append table: {table}\")\n",
    "    (df.write\n",
    "     .format(\"delta\")\n",
    "     .mode(mode)\n",
    "     .option(\"mergeSchema\", \"true\")\n",
    "     .partitionBt(partition)\n",
    "     .saveAsTable(table)\n",
    "    )\n",
    "  \n",
    "  elif mode == \"overwrite\":\n",
    "    print(f\"Overwrite table: {table}\")\n",
    "    (df.write\n",
    "      .format(\"delta\")\n",
    "      .mode(mode)\n",
    "      .option(\"overwriteSchema\", \"true\")\n",
    "      .partitionBt(partition)\n",
    "      .saveAsTable(table)\n",
    "    )\n",
    "\n",
    "  elif mode == \"merge\":\n",
    "    deltaTable = DeltaTable.forName(spark, table)\n",
    "    print(f\"Merge table: {table}\")\n",
    "    (deltaTable.alias(\"target\")\n",
    "     .merge(df.alias(\"update\"), f\"target.{pk_col} = update.{pk_col}\")\n",
    "     .whenMatchedUpdateAll()\n",
    "     .whenNotMatchedInsertAll()\n",
    "     .execute\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection=\"sales\"\n",
    "\n",
    "pipeline=\"[{'$match': { 'items.name':'printer paper' }}, {'$unwind': { path: '$items' }}, {'$addFields': { totalSale: { \\\n",
    "\t'$multiply': [ '$items.price', '$items.quantity' ] } }}, {'$project': { saleDate:1,totalSale:1,_id:0 }}]\"\n",
    "\n",
    "salesDF = spark.read.format(\"mongodb\").option(\"database\", database).option(\"collection\", collection).option(\"pipeline\", pipeline).option(\"partitioner\", \"MongoSinglePartitioner\").option(\"spark.mongodb.input.uri\", connectionString).load()\n",
    "display(salesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesDF = master_array(salesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_data(salesDF, collection, \"_id\", \"city\", mode=\"append\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
