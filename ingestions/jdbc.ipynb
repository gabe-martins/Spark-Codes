{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F  \n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = \"postgre\"\n",
    "DEST = \"landing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"jdbc:postgresql://localhost:5432/tabelas\"\n",
    "user = \"postgres\"\n",
    "password =\"123456\"\n",
    "driver = \"org.postgresql.Driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jdbc_data(qry):\n",
    "  jdbc_data = (spark.read.format(\"jdbc\")\n",
    "    .option(\"driver\", driver)\n",
    "    .option(\"url\", url)\n",
    "    .option(\"query\", qry)\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    # .option(\"fetchSize\", \"100000\")\n",
    "    .load()\n",
    "  )   \n",
    "  return jdbc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(df, table, pk_col, partition, mode):\n",
    "  mode = mode.lower()\n",
    "  ingestion_time = datetime.now()\n",
    "  dest_path = f\"{SOURCE}/{DEST}/{table}/{ingestion_time.strftime('%Y%m%d')}\"\n",
    "\n",
    "  df = df.withColumn('INGESTION_TIME', F.lit(ingestion_time))\n",
    "\n",
    "  if mode == \"append\":\n",
    "    print(f\"Append table: {table}\")\n",
    "    (df.write\n",
    "     .format(\"delta\")\n",
    "     .mode(mode)\n",
    "     .option(\"mergeSchema\", \"true\")\n",
    "     .partitionBt(partition)\n",
    "     .saveAsTable(table)\n",
    "    )\n",
    "  \n",
    "  elif mode == \"overwrite\":\n",
    "    print(f\"Overwrite table: {table}\")\n",
    "    (df.write\n",
    "      .format(\"delta\")\n",
    "      .mode(mode)\n",
    "      .option(\"overwriteSchema\", \"true\")\n",
    "      .partitionBt(partition)\n",
    "      .saveAsTable(table)\n",
    "    )\n",
    "\n",
    "  elif mode == \"merge\":\n",
    "    deltaTable = DeltaTable.forName(spark, table)\n",
    "    print(f\"Merge table: {table}\")\n",
    "    (deltaTable.alias(\"target\")\n",
    "     .merge(df.alias(\"update\"), f\"target.{pk_col} = update.{pk_col}\")\n",
    "     .whenMatchedUpdateAll()\n",
    "     .whenNotMatchedInsertAll()\n",
    "     .execute\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"people\"\n",
    "pk_col = \"id\"\n",
    "qry = f\"select * from {table} where {pk_col} >= 1000\"\n",
    "df = get_jdbc_data(qry)\n",
    "ingest_data(df, table, pk_col, \"city\", mode=\"append\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
